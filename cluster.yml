AWSTemplateFormatVersion: '2010-09-09'
Description: Template for Watsonx governance deployment on Single Node Self-managed Openshift platform (SNO).
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label: 
          default: Network Settings
        Parameters:
          - VPCID
          - NumberOfAZs
          - AvailabilityZones
          - PrivateSubnet1ID
          - PrivateSubnet2ID
          - PrivateSubnet3ID
          - PublicSubnet1ID
          - PublicSubnet2ID
          - PublicSubnet3ID
          - BootNodeAccessCIDR
          - MachineCIDR
          - ClusterNetworkCIDR
          - ClusterNetworkHostPrefix
          - ServiceNetworkCIDR
      - Label: 
          default: Instance Configuration
        Parameters:
          - KeyPairName
      - Label: 
          default: Openshift Configuration
        Parameters:
          - OpenshiftVersion
          - MasterInstanceType
          - MasterVolumeIOPS
          - MasterVolumeSize
          - MasterVolumeType
          - DomainName
          - PrivateCluster
          - EnableFips
          - StorageType
          - ClusterName
          - RedhatPullSecret
      - Label: 
          default: Watsonx Governance
        Parameters:
          - EntitlementKey
          - LicenseAgreement
          - CloudPakVersion
Parameters:
  ClusterName:
    Default: "sno-wxgov"
    Description: Custom cluster name for kubernetes.io/cluster/tags.
    Type: String
    AllowedPattern: ^[0-9a-z-]*$
  DomainName:
    Description: 'Amazon Route53 base domain configured for your OpenShift Container Platform cluster. Name must consist of lower case alphanumeric characters and must start and end with an alphanumeric character. ex. ibmworkshops.com'
    Type: String
    Default: ""
  PrivateCluster:  
    Description: To Deploy a Private cluster select true and false for Public cluster
    Type: String
    AllowedValues:
      - "True"
      - "False"
    Default: "False"
  RedhatPullSecret:
    Description: Your Red Hat Network (RHN) pull secret(e.g., s3://my-bucket/path/to/pull_secret.json).
    Type: String
  OpenshiftVersion:
    Description: Choose Openshift Version
    Type: String
    Default: "4.15.28"
  EnableFips:
    Description: Enable Fips for Openshift
    Type: String
    AllowedValues:
      - "false"
      - "true"
    Default: "false"
  MasterInstanceType:
    Default: m6i.4xlarge
    AllowedValues:
      - m6i.4xlarge
      - m6i.8xlarge
      - m6i.12xlarge
      - m6i.24xlarge
    ConstraintDescription: Must contain valid instance type
    Description: The EC2 instance type for the OpenShift master instances.
    Type: String
  MasterVolumeIOPS:
    Default: 4000
    Description: The ControlPlane volume input/output operation per seconds. For more details, refer AWS documentation
    ConstraintDescription: Must contain Integer value
    Type: Number
  MasterVolumeSize:
    Default: 400
    Description: The ControlPlace volume size
    ConstraintDescription: Must contain Integer value
    Type: Number
  MasterVolumeType:
    Default: gp3
    Type: String
    Description: The ControlPlace volume type.
    AllowedValues:
      - gp3
      - gp2
      - io1
      - io2
  StorageType:
    Description: Choose storage type to be configured.
    Type: String
    AllowedValues:
      - "EFS"
    Default: "EFS"
  NumberOfAZs:  
    Default: 1
    Description: >-
      The number of Availability Zones to be used for the deployment. Keep in mind that some regions may be limited to two Availability Zones. For a cluster to be highly available, three Availability Zones are needed to avoid a single point of failure.
    Type: Number
    AllowedValues:
    - 1
    - 3  
  AvailabilityZones:
    Description: The list of Availability Zones to use for the subnets in the VPC. The Template uses one or three Availability Zones and preserves the logical order you specify.
    Type: List<AWS::EC2::AvailabilityZone::Name>
    Default: us-east-2a
  VPCID:
    Description: The ID of your existing VPC for deployment.
    Type: AWS::EC2::VPC::Id
  PrivateSubnet1ID:
    Description: The ID of the private subnet in Availability Zone A for the workload (e.g., subnet-a0246dcd).
    Type: String
    Default: ""
  PrivateSubnet2ID:
    Description: The ID of the private subnet in Availability Zone B for the workload (e.g., subnet-b1f432cd).
    Type: String
    Default: ""
  PrivateSubnet3ID:
    Description: The ID of the private subnet in Availability Zone C for the workload (e.g., subnet-b1f4a2cd).
    Type: String
    Default: ""
  PublicSubnet1ID:
    Description: The ID of the public subnet in Availability Zone A for the ELB load balancer (e.g., subnet-9bc642ac).
    Type: String
    Default: ""
  PublicSubnet2ID:
    Description: The ID of the public subnet in Availability Zone B for the ELB load balancer (e.g., subnet-e3246d8e).
    Type: String
    Default: ""
  PublicSubnet3ID:
    Description: The ID of the public subnet in Availability Zone C for the ELB load balancer (e.g., subnet-e324ad8e).
    Type: String
    Default: ""
  BootNodeAccessCIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/x
    Description: The CIDR IP range that is permitted to access boot node instance. We recommend that you set this value to a trusted IP range. The value `0.0.0.0/0` permits all IP addresses to access. Additional values can be added post-deployment from the Amazon EC2 console.
    Type: String
    Default: 0.0.0.0/0
  KeyPairName:
    Description: The name of an existing public/private key pair, which allows you
      to securely connect to your instance after it launches.
    Type: AWS::EC2::KeyPair::KeyName
  ClusterNetworkCIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16-28
    Default: 10.128.0.0/14
    Description: The Cluster Network CIDR IP range that is used as IP address pools for pods.
    Type: String
  ClusterNetworkHostPrefix:
    Description: Cluster network host prefix.
    Type: Number
    Default: 23    
  MachineCIDR:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/(1[6-9]|2[0-8]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16-28
    Default: 10.0.0.0/16
    Description: The CIDR block of the existing VPC. It must be match for subnets
    Type: String
  ServiceNetworkCIDR: 
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Description: The service network CIDR IP range.
    Type: String
    Default: 172.30.0.0/16
  EntitlementKey:
    Description: Entitlement key for the IBM Entitled Registry, used to pull the software images.
    Type: String 
    AllowedPattern: \S+
    ConstraintDescription: EntitlementKey must have value
    NoEcho: 'true'
  LicenseAgreement:
    Description: By accepting the license agreement you are accepting the terms and conditions of the license.  For more information about licenses, click this link https://ibm.biz/int-licenses
    Type: String
    Default: 'reject'
    AllowedValues:
      - "accept"
      - "reject"
    ConstraintDescription: must answer 'accept'
  CloudPakVersion:
    Description: Cloud Pak version.
    Type: String
    Default: "5.0.2"
      
Mappings:
  AWSAMIRegionMap:
    us-east-1:
      BootNodeAmiId: ami-06640050dc3f556bb
    us-east-2:
      BootNodeAmiId: ami-0aa8fc2422063977a
    us-west-1:
      BootNodeAmiId: ami-0186e3fec9b0283ee
    us-west-2:
      BootNodeAmiId: ami-08970fb2e5767e3b8
    ap-south-1:
      BootNodeAmiId: ami-05c8ca4485f8b138a
    ap-northeast-3:
      BootNodeAmiId: ami-044921b7897a7e0da
    ap-northeast-2:
      BootNodeAmiId: ami-06c568b08b5a431d5
    ap-southeast-1:
      BootNodeAmiId: ami-051f0947e420652a9
    ap-southeast-2:
      BootNodeAmiId: ami-0808460885ff81045
    ap-northeast-1:
      BootNodeAmiId: ami-0f903fb156f24adbf
    ca-central-1:
      BootNodeAmiId: ami-0c3d3a230b9668c02
    eu-central-1:
      BootNodeAmiId: ami-0e7e134863fac4946
    eu-west-1:
      BootNodeAmiId: ami-0f0f1c02e5e4d9d9f
    eu-west-2:
      BootNodeAmiId: ami-035c5dc086849b5de
    eu-west-3:
      BootNodeAmiId: ami-0460bf124812bebfa
    eu-north-1:
      BootNodeAmiId: ami-06a2a41d455060f8b
    sa-east-1:
      BootNodeAmiId: ami-0c1b8b886626f940c
    ap-east-1:
      BootNodeAmiId: ami-011a403f2a9b2c39f

Rules: 
  SubnetsInVPC:
    Assertions:
      - Assert: !EachMemberIn
          - !ValueOfAll
            - AWS::EC2::Subnet::Id
            - VpcId
          - !RefAll 'AWS::EC2::VPC::Id'
        AssertDescription: All subnets must in the VPC      
Conditions:
  3AZCondition: !Equals [!Ref NumberOfAZs, 3]
Resources:


  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AdministratorAccess        
      Path: /
      Policies:
        - PolicyName: lambda-cleanUpLambda
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ssm:SendCommand
                  - ssm:PutParameter
                  - ssm:GetParameter
                  - ssm:DeleteParameter
                Resource:
                  - '*'
              - Effect: Allow
                Action:
                  - logs:FilterLogEvents
                Resource:
                  - '*' 

  BootNodeIamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Effect: "Allow"
          Principal:
            Service:
            - "ec2.amazonaws.com"
          Action:
          - "sts:AssumeRole"
        - Effect: "Allow"
          Principal:
            AWS:
            - Ref: AWS::AccountId
          Action:
          - "sts:AssumeRole"
      MaxSessionDuration: 43200
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/AWSCloudFormationReadOnlyAccess
        - arn:aws:iam::aws:policy/AdministratorAccess        
      Policies:
      - PolicyName: bootnode-policy
        PolicyDocument:
          Version: "2012-10-17"
          Statement:
          - Effect: "Allow"
            Action: "ec2:Describe*"
            Resource: "*"
          - Effect: "Allow"
            Action: "ec2:AttachVolume"
            Resource: "*"
          - Effect: "Allow"
            Action: "ec2:DetachVolume"
            Resource: "*"
          - Effect: "Allow"
            Action: "route53:*"
            Resource: "*"
          - Effect: "Allow"
            Action:
            - "secretsmanager:GetSecretValue"
            - "secretsmanager:UpdateSecret"
            - "secretsmanager:CreateSecret"
            Resource: "*"
          - Effect: Allow
            Action:
            - ssm:SendCommand
            - ssm:PutParameter
            - ssm:GetParameter
            Resource:
            - '*'  

  BootnodeInstanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: "/"
      Roles:
      - Ref: "BootNodeIamRole"

  BootnodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Cluster Bootnode Security Group
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: !Ref BootNodeAccessCIDR
      VpcId: !Ref VPCID

  BootnodeInstance:
    Type: AWS::EC2::Instance
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          Required:
            - StackPropertiesFile
        StackPropertiesFile:
          files:
            /root/mystack.props:
              content: !Sub |
                AWS_REGION=${AWS::Region}
                AWS_STACKID="${AWS::StackId}"
                AWS_STACKNAME="${AWS::StackName}"
              mode: '000644'
              owner: root
              group: root
            /home/ec2-user/destroy.sh:
              content: !Sub |
                echo "$1 - Destroy"
                export HOME=/home/ec2-user
                cd $HOME/installer
                sudo openshift-install destroy cluster > $HOME/destroy.log
                echo "Destroy completed"
                aws ssm put-parameter --name $1"_CleanupStatus" --type "String" --value "READY" --overwrite
              mode: '000755'
              owner: root
              group: root
            /root/.aws/config:
              content: !Sub |
                [default]
                region=${AWS::Region}
              mode: '000600'
              owner: root
              group: root

    Properties:
      KeyName: !Ref 'KeyPairName'
      ImageId: !FindInMap [AWSAMIRegionMap, !Ref "AWS::Region", BootNodeAmiId]
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: 50
            VolumeType: gp3  
      IamInstanceProfile: !Ref BootnodeInstanceProfile
      Tags:
        - Key: Name
          Value: 
            !Sub
            - "${ClusterName}-bootnode"
            - ClusterName: !Ref ClusterName 
      InstanceType: t3.large 
      NetworkInterfaces:
      - GroupSet:
        - !Ref BootnodeSecurityGroup
        AssociatePublicIpAddress: true
        DeviceIndex: '0'
        DeleteOnTermination: true
        SubnetId: !Ref PublicSubnet1ID  
      UserData:
        Fn::Base64:
          !Sub 
          - |
            #!/bin/bash -x
            
            #include required tools for the installation
            yum update -y
            yum install -y git podman wget jq python3.12 unzip
            podman version

            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            ./aws/install &> /var/log/userdata.awscli_install.log

            git clone  https://github.com/aws-quickstart/quickstart-linux-utilities.git
            sed -i "s/aws-cfn-bootstrap-latest.tar.gz/aws-cfn-bootstrap-py3-latest.tar.gz/g" /quickstart-linux-utilities/quickstart-cfn-tools.source 
            export P=/quickstart-linux-utilities/quickstart-cfn-tools.source
            source $P
            
            qs_bootstrap_pip || qs_err " pip bootstrap failed "
            qs_aws-cfn-bootstrap || qs_err "cfn bootstrap failed"
            
            #pip3 install awscli  &> /var/log/userdata.awscli_install.log || qs_err " awscli install failed "
            /usr/local/bin/cfn-init -v --stack ${AWS::StackName} --resource BootnodeInstance --configsets Required --region ${AWS::Region}
            sudo cp /usr/local/bin/aws /usr/bin/aws

            pip3 install ansible 

            cd /tmp
            qs_retry_command 10 wget https://s3-us-west-1.amazonaws.com/amazon-ssm-us-west-1/latest/linux_amd64/amazon-ssm-agent.rpm
            qs_retry_command 10 yum install -y ./amazon-ssm-agent.rpm
            systemctl start amazon-ssm-agent
            systemctl enable amazon-ssm-agent
            rm -f ./amazon-ssm-agent.rpm

            wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
            chmod a+x /usr/local/bin/yq

            wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/${OpenshiftVersion}/openshift-install-linux-${OpenshiftVersion}.tar.gz
            tar -xvzf openshift-install-linux-${OpenshiftVersion}.tar.gz
            cp openshift-install /usr/local/bin/
            cp openshift-install /usr/bin/

            wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/${OpenshiftVersion}/openshift-client-linux-${OpenshiftVersion}.tar.gz
            tar -xvzf openshift-client-linux-${OpenshiftVersion}.tar.gz
            cp oc /usr/local/bin/
            cp oc /usr/bin/
            cp kubectl /usr/local/bin/
            cp kubectl /usr/bin/

            export ICPDInstallationCompletedURL='${ICPDInstallationCompletedHandle}'
            export AWS_REGION=${AWS::Region}
            export AWS_STACKID=${AWS::StackId}
            export AWS_STACKNAME=${AWS::StackName}

            # Installing OpenShift
            export HOME=/home/ec2-user
            cd $HOME

            # change mode of destroy script
            chmod +x $HOME/destroy.sh

            # generate public key
            ssh-keygen -t rsa -b 4096 -f $HOME/.ssh/id_rsa -q -N ""
            
            # configure installer
            mkdir -p installer
            cd installer

            # copy pull-secret from S3
            aws s3 cp ${RedhatPullSecret} pull-secret

            export RELEASE_IMAGE=$(openshift-install version | awk '/release image/ {print $3}')
            echo $RELEASE_IMAGE

            export CCO_IMAGE=$(oc adm release info --image-for='cloud-credential-operator' $RELEASE_IMAGE)
            echo $CCO_IMAGE

            oc image extract $CCO_IMAGE --file="/usr/bin/ccoctl" -a ./pull-secret
            chmod 755 ccoctl
            cp ccoctl /usr/local/bin/
            cp ccoctl /usr/bin/

            mkdir -p credreqs
            oc adm release extract --cloud=aws --credentials-requests $RELEASE_IMAGE --to=./credreqs

            # Create install-config.yaml
            yq '.apiVersion = "v1"' install-config.yaml > install-config.yaml
            yq -i ".baseDomain = \"${DomainName}\"" install-config.yaml
            yq -i '.credentialsMode = "Manual"' install-config.yaml

            # Create compute node
            yq -i '.compute[0].architecture = "amd64"' install-config.yaml
            yq -i '.compute[0].hyperthreading = "Enabled"' install-config.yaml
            yq -i '.compute[0].name = "worker"' install-config.yaml
            yq -i '.compute[0].platform.aws.rootVolume.iops = 2000' install-config.yaml
            yq -i '.compute[0].platform.aws.rootVolume.size = 250' install-config.yaml
            yq -i '.compute[0].platform.aws.rootVolume.type = "gp3"' install-config.yaml

            # add compute zone for worker
            if [ "${MultiAZ}" == "True" ]; then
              yq -i ".compute[0].platform.aws.zones += [ \"${AvailabilityZone1}\",\"${AvailabilityZone2}\",\"${AvailabilityZone3}\" ]" install-config.yaml
            else
              yq -i ".compute[0].platform.aws.zones += [ \"${AvailabilityZone1}\" ]" install-config.yaml
            fi

            # add worker node count
            export WorkerNodeQuantity=0
            yq -i ".compute[0].replicas = $WorkerNodeQuantity" install-config.yaml

            # control plane
            yq -i '.controlPlane.architecture = "amd64"' install-config.yaml
            yq -i '.controlPlane.hyperthreading = "Enabled"' install-config.yaml
            yq -i '.controlPlane.name = "master"' install-config.yaml

            # add compute zone for master
            if [ "${MultiAZ}" == "True" ]; then
              yq -i ".controlPlane.platform.aws.zones += [ \"${AvailabilityZone1}\",\"${AvailabilityZone2}\",\"${AvailabilityZone3}\" ]" install-config.yaml
            else
              yq -i ".controlPlane.platform.aws.zones += [ \"${AvailabilityZone1}\" ]" install-config.yaml
            fi
            
            yq -i ".controlPlane.platform.aws.rootVolume.iops = ${MasterVolumeIOPS}" install-config.yaml
            yq -i ".controlPlane.platform.aws.rootVolume.size = ${MasterVolumeSize}" install-config.yaml
            yq -i ".controlPlane.platform.aws.rootVolume.type = \"${MasterVolumeType}\"" install-config.yaml
            yq -i ".controlPlane.platform.aws.type = \"${MasterInstanceType}\"" install-config.yaml

            yq -i ".controlPlane.replicas = 1" install-config.yaml


            # metadata
            yq -i ".metadata.name = \"${ClusterName}\"" install-config.yaml

            # networking
            yq -i ".networking.clusterNetwork[0].cidr = \"${ClusterNetworkCIDR}\"" install-config.yaml
            yq -i ".networking.clusterNetwork[0].hostPrefix = ${ClusterNetworkHostPrefix}" install-config.yaml

            yq -i ".networking.machineNetwork[0].cidr = \"${MachineCIDR}\"" install-config.yaml
            yq -i '.networking.networkType = "OVNKubernetes"' install-config.yaml

            yq -i ".networking.serviceNetwork[0] = \"${ServiceNetworkCIDR}\"" install-config.yaml

            yq -i ".platform.aws.region = \"${AWS::Region}\"" install-config.yaml

            # add subnet
            if [ "${MultiAZ}" == "True" ]; then
              if [ "${PrivateCluster}" == "True" ]; then
                yq -i ".platform.aws.subnets += [ \"${PrivateSubnet1ID}\",\"${PrivateSubnet2ID}\",\"${PrivateSubnet3ID}\" ]" install-config.yaml
              else
                yq -i ".platform.aws.subnets += [ \"${PrivateSubnet1ID}\",\"${PrivateSubnet2ID}\",\"${PrivateSubnet3ID}\",\"${PublicSubnet1ID}\",\"${PublicSubnet2ID}\",\"${PublicSubnet3ID}\" ]" install-config.yaml
              fi
            else
              if [ "${PrivateCluster}" == "True" ]; then
                yq -i ".platform.aws.subnets += [ \"${PrivateSubnet1ID}\" ]" install-config.yaml
              else
                yq -i ".platform.aws.subnets += [ \"${PrivateSubnet1ID}\",\"${PublicSubnet1ID}\" ]" install-config.yaml
              fi
            fi

            yq -i ".fips = ${EnableFips}" install-config.yaml

            # sshkey
            export SSHKEY=`cat $HOME/.ssh/id_rsa.pub`
            yq -i ".sshKey = \"$SSHKEY\"" install-config.yaml

            # pull-secret
            yq -i '.pullSecret = "|PULLSECRET|"' install-config.yaml
            export PULLSECRET=`cat $HOME/installer/pull-secret`
            sed -i 's#|PULLSECRET|#'$PULLSECRET'#g' install-config.yaml

            if [ "${PrivateCluster}" == "True" ]; then
              yq -i '.publish = "Internal"' install-config.yaml
            else
              yq -i '.publish = "External"' install-config.yaml
            fi            

            cp install-config.yaml /tmp/

            # generate openshift manifest
            openshift-install create manifests

            # generate IAM roles, ServiceAccount and OIDC
            ccoctl aws create-all --name ${ClusterName} --region ${AWS::Region} --credentials-requests-dir ./credreqs --output-dir _output

            cp _output/manifests/* manifests/
            cp -a _output/tls .

            openshift-install create cluster --log-level=debug
            chown -R ec2-user:ec2-user $HOME/installer
            export KUBECONFIG=$HOME/installer/auth/kubeconfig

            ecode=$?
            echo "Storage type ${StorageType}"

            cd $HOME
            git clone https://github.com/ibmaws/sno-wxgov.git
            cp ./sno-wxgov/scripts/* $HOME/
            chmod +x deploy-wx-gov.sh
            chmod +x efs-storage.sh
            
            ## EFS
            if [ "${StorageType}" == "EFS" ]; then
              echo "EFS is selected as StorageType"
              if [ ${MultiAZ} == "True"  ]; then
                MULTI_AZ="true"
                EFS_SUBNETS=${PrivateSubnet1ID},${PrivateSubnet2ID},${PrivateSubnet3ID}
              else 
                MULTI_AZ="false"
                EFS_SUBNETS=${PrivateSubnet1ID}
              fi
              export HOME=/home/ec2-user

              export CLUSTER_USERNAME=kubeadmin
              export CLUSTER_PASSWORD=$(cat $HOME/installer/auth/kubeadmin-password)
              export CLUSTER_URL=https://api.${ClusterName}.${DomainName}:6443

              /bin/bash $HOME/efs-storage.sh --base-path=$HOME \
              --operation=create \
              --subnets=$EFS_SUBNETS \
              --cluster-username=$CLUSTER_USERNAME \
              --cluster-password=$CLUSTER_PASSWORD \
              --cluster-url=$CLUSTER_URL
            fi   
            ecode=$?
         
            # Setup Watxonx Governance begins

            oc label machineconfigpool master custom-kubelet=large-pods-num
            oc apply -f max-pods-config.yaml

            export IMAGE_ARCH=amd64
            export SERVER_ARGUMENTS="--server=$CLUSTER_URL"
            export LOGIN_ARGUMENTS="--username=$CLUSTER_USERNAME --password=$CLUSTER_PASSWORD"
            export CPDM_OC_LOGIN="cpd-cli manage login-to-ocp $SERVER_ARGUMENTS $LOGIN_ARGUMENTS"
            export OC_LOGIN="oc login $CLUSTER_URL $LOGIN_ARGUMENTS"
            export PROJECT_CERT_MANAGER=ibm-cert-manager
            export PROJECT_LICENSE_SERVICE=ibm-licensing
            export PROJECT_SCHEDULING_SERVICE=cpd-scheduler
            export PROJECT_CPD_INST_OPERATORS=cpd-operators
            export PROJECT_CPD_INST_OPERANDS=cpd
            export STG_CLASS_BLOCK=efs-nfs-client
            export STG_CLASS_FILE=efs-nfs-client
            export IBM_ENTITLEMENT_KEY=${EntitlementKey}
            export VERSION=5.0.2
            export COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_platform,db2oltp,watsonx_governance
            efs_filesystem_id=$(grep '^efs_filesystem_id' $HOME/installer-files/.info | awk '{print $2}')
            export EFS_LOCATION=$efs_filesystem_id.efs.$AWS_REGION.amazonaws.com
            echo $EFS_LOCATION
            export EFS_PATH=/
            export PROJECT_NFS_PROVISIONER=nfs-provisioner
            export EFS_STORAGE_CLASS=efs-nfs-client
            export NFS_IMAGE=k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2

            cd $HOME
            /bin/bash $HOME/deploy-wx-gov.sh \
              --cluster-username=$CLUSTER_USERNAME \
              --cluster-password=$CLUSTER_PASSWORD \
              --cluster-url=$CLUSTER_URL

            /usr/local/bin/cfn-signal --exit-code $ecode --id $AWS_STACKID  --data "See logs at $HOME/cpd-status/log/" $ICPDInstallationCompletedURL
          -
            MultiAZ: !If [ 3AZCondition , 'True', 'False']
            AvailabilityZone1: !Select [0, !Ref AvailabilityZones]
            AvailabilityZone2: !If [ 3AZCondition, !Select [1, !Ref AvailabilityZones], ""]
            AvailabilityZone3: !If [ 3AZCondition, !Select [2, !Ref AvailabilityZones], ""]
           
  CleanUpLambda:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import boto3
          import json
          import cfnresponse
          import os
          import traceback
          import time
          def handler(event, context):
              responseData = {}
              try:
                  print("event_obj:",json.dumps(event))
                  print(event['RequestType'])
                  if event['RequestType'] == 'Delete':
                    print("Run unsubscribe script")
                    ssm = boto3.client('ssm',region_name=os.environ['Region'])
                    instanceID = os.environ['BootNode']
                    stackname = os.environ['StackName']
                    print(instanceID)
                    response = ssm.send_command(Targets=[{"Key":"instanceids","Values":[instanceID]}],
                            DocumentName="AWS-RunShellScript",
                            Parameters={"commands":["/home/ec2-user/destroy.sh %s" %(stackname)],
                                        "executionTimeout":["1200"],
                                        "workingDirectory":["/home/ec2-user"]},
                            Comment="Execute script in uninstall openshift",
                            TimeoutSeconds=120)
                    print(response)
                    current_status = "WAIT"
                    final_status = "READY"
                    parameterName = stackname+"_CleanupStatus"           
                    response = ssm.put_parameter(Name=parameterName,
                           Description="Waiting for CleanupStatus to be READY",
                           Value=current_status,
                           Type='String',
                           Overwrite=True)        
                    print(response)    
                    while(current_status!=final_status):
                      time.sleep(30) 
                      response = ssm.get_parameter(Name=parameterName)
                      parameter = response.get('Parameter')
                      current_status = parameter.get('Value')
                      print(current_status)
                    ssm.delete_parameter(Name=parameterName)    
              except Exception as e:
                print(e)
                traceback.print_exc()
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, '')
      Environment:
        Variables:
          Region: !Ref AWS::Region
          BootNode: !Ref BootnodeInstance
          StackName: !Ref AWS::StackName
      Handler: index.handler
      Role: !GetAtt 'LambdaExecutionRole.Arn'
      Runtime: python3.8
      Timeout: 600
             
  Cleanup :
    Type: Custom::Cleanup
    Properties:
      DependsOn: BootnodeInstance
      ServiceToken: !GetAtt 'CleanUpLambda.Arn'
 
  ICPDInstallationCompletedHandle:
    Type: AWS::CloudFormation::WaitConditionHandle  

  ICPDInstallationCompleted:
    Type: AWS::CloudFormation::WaitCondition
    Properties:
      Count: 1
      Handle: !Ref ICPDInstallationCompletedHandle
      Timeout: '40000'  

Outputs:
  BootnodeInstanceId:
    Description: Bootnode Instance ID.
    Value: !Ref BootnodeInstance

  BootnodePublicIp:
    Description: The boot node public IP address.
    Value: !GetAtt BootnodeInstance.PublicIp
